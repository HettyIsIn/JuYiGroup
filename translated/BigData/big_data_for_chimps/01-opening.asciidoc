== 洞察力从何而来，来源于上下文中的数据

本文开篇完全可以就大数据的庞大性及重要性展开长篇大论——这一点现已能被综合衡量，同时使我们充分认识到大数据的必要性，可是在诸如“观众参与”、“无法预知事件”，“……”这些先前无法量化的特质面前用这番言论未免太过苍白。不过既然您已经在阅读这篇文章，本文将以全新地视角来讲述大数据（随后是“怎样让你的老板认识大数据”和“什么是大数据（‘一个真正好的营销工具’）？”）

现在，让我们来谈谈机器人跟人类。

在 x 年，计算机“深蓝”向卡斯帕罗夫发起了挑战。随后在 y 年,“深蓝”战胜了卡斯帕罗夫，计算机战胜了人类，卡斯帕罗夫不无沮丧地放弃回到了家中。这种营销炒作的大数据使我们获得了无法预知的能力，然而在得到这种能力的时我们很容易犯一个错误，我们利用它、依赖它、离不开它、最终，成为它的奴隶——我们赞同“我们只相信上帝，其他人带来数据”——仅仅只有它能被足够的保真度所量化时它才能被加以利用。

在这次人机大战中，机器人“深蓝”大败加里·卡斯帕罗夫，大数据工具的能力初现端倪

.加里·卡斯帕罗夫, "国际象棋大师对战计算机", 2010
________
在1996年的一场比赛中，我以微弱优势战胜了超级计算机“深蓝”。随后，1997年，IBM加倍投入，使“深蓝”的处理能力翻了一番———由于复赛中的一个失误我丢掉了比赛，全世界争相报导。这样的结果使很多人感到悲痛震惊，他们将其看为是人类向万能电脑俯首称臣的一个信号；（见《新闻周刊》头条：“大脑的最后一站”）。其他人则耸了耸肩,惊讶于人类到1997年还能够与具有无穷计算能力的计算机进行竞争，它们几乎占据了第一世界的所有席位。……无人理解在笔记本上带有一个超级大师所能带来的全部影响，尤其不知道这在职业象棋比赛中到底意味什么。

强大象棋软件快速扩张流行，产生了很多意想不到的结果，正面影响及负面后果皆有之。孩子们喜欢并很自然地接受了计算机，所以勿需惊讶于他们同样接受了计算机与象棋的无缝融合。随着超强软件的引进，年轻人在家就有可能拥有一个顶级的对手，而不是从小就需要一个专业的教练。对于那些只有少数象棋传统以及为数不多教练的国家，现在也能够创造象棋神童了。事实上，我这些年正在训练19岁的马格努斯卡尔森，他是他们中的一员，来自玩象棋相对较少的国家挪威。

计算机分析的深度应用将象棋本身推进向新的方向。机器不关注象棋的走法、棋谱或是数百年来已经建立的学说。它计算出每步棋子的值，分析数十亿计的棋路，然后再重新计算（为了将比赛简化为一堆可动作的数字，计算机将每一个棋子及其位置因子转化成一个值）。它完全免于经验主义及偏见，致力于棋手的发展，通过机器在训练过程中使他们免受经验主义的影响。慢慢地，一步棋是好是坏不能因为这棋看起来是那样或是以前从来没有那种下法来衡量。仅仅只要这步棋有效就称之为好棋，反之，无效就是坏棋。尽管我们仍然需要大力权衡直觉跟逻辑推理来下好一盘棋，但现在越来越多的人类开始像计算机一样下棋。


在数据库中数百万随手可用的游戏同样使最佳玩家越来越年轻化。消化了数千种重要的棋谱及数年来被广泛使用的棋路后，正好验证了马尔科姆·格拉德威尔的那句话“10 000小时成为一个专家”，如同他的新作《局外人》所阐述的那样（他早前的一本著作，《闪烁》，如果能更具创造性地改编，那么更多认知心理题材将在围棋隐喻文化中不断刷新）。如今的年青人，以及幼童，通过植入象棋信息数字档案室并充分利用优越的年青大脑来记住所有的信息，能够增速棋手养成。在无电脑的时代，年青的大师很稀少，而他们也几乎总是注定要为世界冠军而战。博比·菲舍尔作为1958年记录的保持者，他早在15岁获得了大师称号，然而这个记录仅仅在1991就被打破。随后，该记录被一次次刷新，共有20次。现在世界记录的保持者是乌克兰的谢尔盖卡札金，被称之为最年轻的大师，他在2002年以12岁这个悖理的年纪刷新了该记录。现在他20岁了，作为最佳棋手的一员，然而就像许多当下的神童青年一样，他不是领军者费希尔——费希尔很快足够征服象棋世界的每个角落。

如同象棋比赛，在很多事情中，计算机所擅长的正是人类所欠缺的，反之亦然，瑞斯卡-古特曼将其解释为莫拉维克悖论。我灵光乍现，想起了一个实验，如果下棋的时候，机器跟人不是对手而是伙伴，会发生什么呢？以此之长，补彼之短，这样的组合是不是就会无往不胜了？

拥有一个计算机小伙伴同样意味着你决不会担心犯战术失误，计算机能预测出走任意一步棋所能带来的影响，指出其能产生的后果及本来可能不被我们采纳的对策。在这样的配合下，我们不需要花费那么多的时间进行计算，从面能够集中精力进行战略规划，人类的创造力在这样的条件下更显得尤为重要。一个月以前，我在一场常规赛中以4-0战胜了一个保加利亚人。在先前比赛中，我们以3-3平局结束比赛，这次之所以能得胜，我的优势在于计算机帮我分担了策略计算的工作。

2005年，在线象棋游戏网站Playchess.com发布了一个被称之为“自由范”的象棋比赛，棋队中的任何棋手都能向其他棋手或是计算机发起挑战。……由大师级棋手组成的几组棋队跟几个计算机搭档，一同进入了比赛。最初似乎能够预见到比赛的结果，由棋手跟计算机组成的棋队压制了纯计算机棋队，甚至于更高性能的计算机棋队，这台顶级象棋计算机与使用性能较低计算机的大师相较量，最终败下阵来。在象棋界，人类对战略的指导力与计算机对战术的敏锐性相结合，绝对可以称霸群雄、横扫千军、战无不胜。

人们惊讶于这样的结果，获胜方不是一位拥有最高技术水平计算机的大师，而是一队同时使用3台计算进行比赛的美国象棋爱好者，他们操纵、“指导”计算机，使之能深度综观全盘，有效地抵制了获败方的较高认知水平。次等棋手 + 机器 + 稍高性能的棋队优于一台单独的高性能计算机，更无需说特等棋手 + 机器+ 次等性能的棋队了。 http://www.nybooks.com/articles/archives/2010/feb/11/the-chess-master-and-the-computer/
________

本书意在使你成为一个如同此般的专业教练。你不需要成为统计学的泰斗；你不需要成为一个编程专家，我们偏爱于简短精练可读性高的脚本；你不需要在数据库上达到第三形态闪电龙的水准，你只需知道数据怎样运作，如果你能预测执行过程，你能知道什么时候投资，什么时候改进，什么时候有趣的事情将会发生，更重要的是，你将知道得到自己想要的数据要怎样进行评估。（The goal of this book is that you become just such an expert coach. You don't need to be a grandmaster in statistics, have
What you do need is intuition about how to
You don't need to be an expert programmer. We favor short, elegant readable scripts
You don't need to have reached the third dan of dragon-lightning form in database
What you need is intuition about how data moves around
If you can predict the execution, you can know when to invest in improving it and when something funny is going on
Strategic execution
More importantly know how to turn the measurements you have into the data you need
How to augment）

本书将告诉读者怎样去训练计算机，怎样运用卓越的技术。

我们以 “ 机器人为辅，人类为主 ” 为原则（关于从冰箱得到苏打水的数学，关于在云中运行一台计算机的数学）（We have a principle "Robots are cheap, Humans are important,(Math about getting soda from the fridge, about running a computer in the cloud)
）


我们以演示Hadoop内在机制作为开篇，精确却又恰恰点到为止地让你理解数据怎样运作。在大数据环境中，数据的行为（而不是CPU）几乎总是占主导地位，同样计算容量所产生的开销也几乎总是约束着计算的性能。

大数据的一个好处在于其性能的预测原始而又分明 -- ...
（坏处就是它不可能有坏的时候）

一旦读者对正在发生的事情产生了生理直觉，我们将转移到战术领域。我们参考领先的SQL手册来匹配已定义并经过数十年使用实践的模型（贸易的技术）（We consulted the leading SQL cookbooks to find what patterns of use(And tricks of the trade) decades of practice have defined.）抓着"NoSQL"不放，抛开传统知识不提总不会是一个好的计划。

// 四个层级：解释，优化，预测，控制（运筹学博客）



跟踪你每一次的交货方式使车队提高燃油使用率，既能确保司机跟乘客的安全又提高操作效率，降低成本。





// 深度阅读: 一个JT＆南妮特会议的插曲 (未来闪影)

数据是毫无价值的。事实上，它比毫无价值更糟糕：它需要金钱跟精力去采集、存储、传输及组织，没有人想要它。

有价值的是 _洞察力_ -- 总结、模型及关系将引导我们拥有更深的理解跟更好的决定。同时，洞察力来源于上下文中数据合成。我们能够通过定位商业航班的到达时间及其上下文中以小时计的全球天气数据来预测航班延误（具体算法见本章参考）。捕获某大型网站的日志文件中的事件高峰期，使用由用户浏览网站的路径所定义的上下文对其重组，你将能叙述与之相类似的兴趣的文章（见本章（参考））。在本章（参考），我们将对维基百科中的每一个文本进行拆解，然后重组词汇，将其从每一个实际存在的文章中重组至一个被主题定位所定义一个文本组中 – 将洞察力转化为人类语言，否则无法对其量化。

在每一个这样的示例内部都有两个相矛盾的抗力，它们相互作用促进开路事物的发展，使数据脱离传统数据分析的统治进而转入本书的主题：“大数据”分析及简易分析。数据容量的限制，使如此之大的数据量无法方便地在单个机器上进行分析；同样，数据的综合性，在小规模范围内能够抽取样本的简单策略如同沧海一粟，起不到作用。

=== 大数据 : 解决综合数据危机的工具

让我们来做一个超基础的分析作业:计数。在州议会为投票表决一项法案计数，或是为订购各种类型的披萨计数，我们在确定的时间将相关人员聚集到同一个房间，进行一次普查，其投票人流走向简单而又快速。

然而，用这种方法不可能计数美国总统的投票表决。没有大到可容纳300人的会议厅；即使有，也没有足够宽的道路让人们顺畅通行；甚至于选举者的出生率或死亡率能与这持续进行中的选举率相拼。

一旦合成所需的数据容量超出了某些可用预算的关键阈值—— 有限的存储空间，有限的网络带宽，有限的时间去准备一个相关反馈，或如此这般 —— 你必须一次又一次地从根本上改变你从数据中提取见解的方法。


我们进行了一次总统选举，将人们聚集到每个当地的投票站进行投票。多地域的分布使得投票者不需要走太远就能参与投票，同时选举的规模使得选举人流走向能保持简单而又快速。当这一天结束，我们对每一个投票站的票额进行合计，将其送到州选举部，选举办公室的官员将各投票站的选票计入结果中以形成最终的选举结果。这种新的途径并没有完全抛弃这种简单快速的方法（将人们聚集到同一个物理位置），引申应用了另一种本地策略（数表内求和）。这是一曲将一个数据采集舞台，高效数据传送舞台，达成一个正确结果的最终报表舞台的管弦乐编曲，人和数据的容量决不会超过其可高效处理的数据量。

因此我们是对该危机的回答正是对大数据的第一个定义：“实用数据分析工具及流程的集合，这个集合不断扩张，甚至随着进行正当合成的数据容量超越可用预算的某些阈值而不断增大。”

// 在第六章（参考）中，我们将在大数据生态系统中绘制出各丰富多彩的工具,
// Hadoop是高水平进行数据批处理普及性选择。
// 工具可以用来了解你制造机的数据模型以确定这个商品是否有缺陷需要在几个月后返厂，或是患者术后的医药记录模型以确定他们患并发病而入院的可能性。



=== Big Data: Tools to Solve the Crisis of Comprehensive Data

Let's take an extremely basic analytic operation: counting. To count the votes for a bill in the state legislature, or for what type of pizza to order, we gather the relevant parties into the same room at a fixed time and take a census of opintions. The logistics here are straightforward.

It is impossible, however, to count votes for the President of the United States this way. No conference hall is big enough to hold 300 million people; if there were, no roads are wide enough to get people to that conference hall; and even still the processing rate would not greatly exceed the rate at which voters come of age or die.

Once the volume of data required for synthesis exceeds some key limit of available computation -- limited memory, limited network bandwith, limited time to prepare a relevant answer, or such -- you're forced to fundamentally rework how you synthesize insight from data.

We conduct a presidential election by sending people to local polling places, distributed so that the participants to not need to travel far, and sized so that the logistics of voting remain straightforward. At the end of day the vote totals from each polling place are summed and sent to the state Elections Division. The folks in the Elections Division office add the results from each polling place to prepare the final result. This new approach doesn't completely discard the straightforward method (gathering people to the same physical location) that worked so well in the small. Instead, it applies another local method (summing a table of numbers). The orchestration of a gathering stage, an efficient data transfer stage, and a final tabulation stage arrives at a correct result, and the volume of people and data never exceeds what can be efficiently processed.

So our first definition of Big Data is a response to a crisis: "A collection of practical data analysis tools and processes that continue to scale even as the volume of data for justified synthesis exceeds some limit of available computation".

// In Chapter 6 (REF) we'll map out the riotous diversity of tools in the Big Data ecosystem,
// Hadoop is the ubiquitous choice for processing batches of data at high
// Hadoop is the tool to use when you want to understand how patterns in data from your manufacturing devices corresponds to defective merchandise returned months later, or how patterns in patients' postoperative medical records correspond to the likelihood they'll be re-admitted with complications.

=== Big Data: Algorithms to Capitalize on the Opportunity of Comprehensive Data

The excitement around Big Data is more than you could explain as "like databases, but bigger". Those tools don't just unlock a new region of scalability, they enable transformative new capabilities.

The data that's powering this revolution isn't just comprehensive, it's _connected_. When your one-in-a-thousand events manifest in sample of ten thousand records, it's noise. When they manifest in ten million records, tiny coincidences reinforce each other to produce patterns. The website etsy.com (an open marketplace for handcrafted goods) has millions of records showing which handcrafted goods people browse and buy. And thanks to their Facebook app they have access to millions of people who have shown interest in those handcrafted goods. Thanks to Facebook's data, they have as well the overlapping interests of those potential customers: "surfing", "big data", "barbeque". Now work backwards. From each interest, find the customers, and from the customers find the purchases, and from the purchase find the categories. What comes forth are unmistakeable patterns such as "People who like the band Lynrd Skynrd are overwhelmingly more likely to purchase Taxidermy". Etsy can better connect people with the things they love, their sellers can better connect with a their fans, and southern-fried rockers can accessorize their living room with that elk's head they always wanted.

Here's what's surprising and important: the algorithms to expose these patterns are not specific to e-commerce, and don't require coming in with guesses about the associations to draw. The work proceeds in three broad steps: (a) provide comprehensive data, identifying its features and connectivity; (b) apply generic methods that use only those features and connectivity (and not a domain-specific model), to expose patterns in the data; (c) interpret those patterns back into the original domain.

This does _not_ follow the accepted path to truth, namely the Scientific Method. Roughly speaking, the scientific method has you (a) use a simplified model of the universe to make falsifiable predictions; (b) test those predictions in controlled circumstances; (c) use established truths to bound any discrepancies footnote:[plus (d) a secret dose of our sense of the model's elegance]. Under this paradigm, data is non-comprehensive: scientific practice demands you carefully control experimental conditions, and the whole point of the model is to strip out all but the reductionistically necessary parameter. A large part of the analytic machinery acts to account for discrepancies from sampling (too little comprehensiveness) or discrepancies from "extraneous" effects (too much comprehensiveness). If those discrepancies are modest, the model is judged to be valid.

This new path to truth is what Peter Norvig (Google's Director of Research) calls "http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/35179.pdf[The unreasonable effectiveness of data]". You don't have to start with a model and you don't necessarily end up with a model. There's no simplification of the universe down to a smaller explanation you can carry forward. Sure, we can apply domain knowledge and say that the correspondence of Lynrd Skynrd with Taxidermy means the robots have captured the notion of "Southern-ness". But for applying the result in practice, there's no reason to do so. The algorithms have replaced a uselessly complicated thing (the trillions of associations possible from interest to product category) with an _actionably_ complicated thing (a scoring of what categories to probabilistically present based on interest). You haven't confirmed a falsifiable hypothesis. But you can win at the track.

The proposition that the Unreasonaly-Effective Method is a worthwhile rival to the Scientific Method is sure to cause barroom brawls at scientific conferences for years to come. This book will not go deeply into advanced algorithms, but we will repeatedly see examples of Unreasonable Effectiveness, as the data comes forth with patterns of its own.

=== The Answer to the Crisis

One solution to the big data crisis is high-performance supercomputing (HPC): push back the limits of computation with brute force. We could conduct our election by gathering supporters of one candidate on a set of cornfields in Iowa, supporters of the other on cornfields in Iowa, and using satellite imaging to tally the result. HPC solutions are exceptionally expensive, require the kind of technology seen only when military and industrial get complex, and though the traditional "all data is local" methods continue to work, they lose their essential straightforward flavor. A supercomputer is not one giant connected room, it's a series of largish rooms connected by very wide multidimensional hallways; HPC programmers have to constantly think about the motion of data among caches, processors, and backing store.

The most important alternative to the HPC approach is the big data tool http://hadoop.apache.org[Hadoop]
which effectively takes the opposite approach. Instead of full control over all aspects of computation and the illusion of data locality, Hadoop revokes almost all control over the motion of data.  Furthermore, unlike the HPC solutions of yore, Hadoop runs on commodity hardware and addresses a wide range of problem domains (finance, medicine, marketing; images, logfiles, mathmatical computation). This power comes at a cost, though. Hadoop understands only a limited vocabulary known as Map/Reduce, and you'll need to learn that vocabulary if Hadoop is to do any work for you.

To get a taste of Map/Reduce, imagine a publisher that banned all literary forms except the haiku:

[verse, The Map/Reduce Haiku]
____________________________________________________________________
data flutters by
    elephants make sturdy piles
  context yields insight
____________________________________________________________________

Our Map/Reduce haiku illustrates Hadoop's template:

1. The Mapper portion of your script processes records, attaching a label to each.
2. Hadoop assembles those records into context groups according to their label.
3. The Reducer portion of your script processes those context groups and writes them to a data store or external system.

While it would be unworkable to have every novel, critical essay, or sonnet be composed of haikus, map/reduce is surprisingly more powerful. From this single primitive, we can construct the familiar relational operations (such as GROUPs and ROLLUPs) of traditional databases, many machine-learning algorithms, matrix and graph transformations and the rest of the advanced data analytics toolkit.

In the coming chapters, we'll walk you through Map/Reduce in its pure form.  We recognize that raw Map/Reduce can be intimidating and inefficient to develop, so we'll also spend a fair amount of time on Map/Reduce abstractions such as Wukong and Pig.

Wukong is a thin layer atop Hadoop using the Ruby programming language. It's the most easily-readable way for us to demonstrate the patterns of data analysis, and you will be able to lift its content into the programming language of your choice footnote:[In the spirit of this book's open-source license, if an eager reader submits a "translation" of the example programs into the programming language of their choice we would love to fold it into in the example code repository and acknowledge the contribution in future printings.]. It's also a powerful tool you won't grow out of.

The high-level Pig programming language has you describe the kind of full-table transformations familiar to database programmers (selecting filtered data, groups and aggregations, joining records from multiple tables). Pig carries out those transformations using efficient map/reduce scripts in Hadoop, based on optimized algorithms you'd otherwise have to reimplement or do without. To hit the sweet spot of "common things are simple, complex things remain possible", you can extend Pig with User-Defined Functions (UDFs), covered in chapter (REF).

This book's code will be roughly 30% Wukong, 60% Pig, and 10% using Java to extend Pig.

Let's take a quick look at some code to compare the two tools.

First, here's a Wukong script.  Don't worry about understanding it in full; just try to get a feel for the flow.

    # CODE validate script, column number, file naming
    cat ufo_sightings.tsv		      | \
      egrep "\w+\tUnited States of America\t" | \
      cut -f 11				      | \
      sort				      | \
      uniq -c > /tmp/state_sightings_ct_sh.tsv

    SELECT COUNT(*), `state`
      FROM `ufo_sightings`.sightings ufos
      WHERE (`country` = 'United States of America') AND (`state` != '')
      GROUP BY `ufos`.`state`
      INTO OUTFILE '/tmp/state_sightings_ct_sql.tsv';

    outfile = File.open('/tmp/state_sightings_ct_rb.tsv', "w");
    File.open('ufo_sightings.tsv').
      select{|line| line =~ /\w+\tUnited States of America\t/ }.
      map{|line| line.split("\t")[10] }.
      sort.chunk(&:to_s).
      map{|key,grp| [grp.count, key] }.
      each{|ct,key| outfile << [ct, key].join("\t") << "\n" }
    outfile.close

We simply _load_ a table, _project_ one field from its contents, _sort_ the values (and in so doing, group each state name's occurrences in a contiguous run), _aggregate_ each group into value and count, and _store_ them into an output file.

----
    mapper(:tsv) do |_,_,_,_,_,_,_,_,_,state,country,*_|
      yield state if country = "United States of America"
    end

    reducer do |state, grp|
      yield [state, grp.count]
    end
----

Here's a similar operation using Pig:

----
    sightings          = load_sightings();
    sightings_us       = FILTER sightings BY (country == 'United States of America') AND (state != '');
    states             = FOREACH sightings_us GENERATE state;
    state_sightings_ct = FOREACH (GROUP states BY state)
      GENERATE COUNT_STAR(states), group;
    STORE state_sightings_ct INTO '$out_dir/state_sightings_ct_pig';
----

=== 三剑客: 批量, 流式, 缩放
早期，我们把洞察力定义为更深层的认识和更好的决策。
Hadoop处理任意规模数据的能力，加上我们提高企业方方面面的综合仪器实力，
代表着在揭露模式上有了根本好转，也说明了人力在模式开发上能达到的范围。

但当Hadoop组织的科研成果开始取得成功时，发生了一件有趣的事情：
研发人员意识到，他们不是只想对模式有更深的理解，
他们希望对这些模式采取行动并且快速作出决定。
工厂老板想要停止生产线，当信号预测随后会出现缺陷时；
医院会希望有社工跟进，当病人不喜欢吃手术后药物时。
必须要及时，因此一个显着的新功能已经进入大数据工具集的核心：流分析（趋势分析）。

流分析获取你_相关的快速洞察分析_，交给Hadoop做深层的全局洞察分析。
Storm+Trident（完全领先的工具集）能够处理低延迟和异常产出的数据；
它可以在Java、Ruby和更多环境下执行复杂的处理;
它可以支持远程APIs或者高并发数据库。


// It's an analytic platform that should be regarded as an essential counterpart to Hadoop and scalable data stores.
// On way to think of Trident is as a tool to do your query on the way _in_ to the database. Rather than insisting every application use the same database and same data model,

三剑客 -- 批量分析、流分析、可伸缩的数据存储 --
是大数据工具集的三个支柱。
他们在一起，能够让你在毫秒时间内分析 terabytes和petabytes
的海量数据，当然也包括数据源杂乱分布在各处时。



=== 分组和排序: 用Pig来分析目击UFO

虽然这些事件令人尴尬的相似，只有采用映射任务(map-only jobs)才有作用，
但Hadoop在涉及到数据集的过滤、分组、统计条目依然表现出众。
我们可以应用这些技术来构建整个美国大陆UFO目击旅行指南。

因为上个示例我们使用了wukong框架，
所以这次我们将使用另外一个叫 Pig 的Hadoop抽象工具。footnote:[http://pig.apache.org]
Pig最声名鹊起的是它给你完整Hadoop能力，
并且使用一个能让你关注数据关系的语法来代替原生的映射和分解操作。

示例数据，包括本书附带的数据集都来自 http://www.infochimps.com/datasets/60000-documented-ufo-sightings-with-text-descriptions-and-metada[美国国家飞碟报告中心]，
包含了超过60,000个UFO目击事件记录。footnote:[对于我们而言，
尽管六万记录太小不足以证明Hadoop拥有的能力，
但这算是一个学习的完美大小了。] 现在不得不痛心地说，许多目击报告都可能是假的，我们需要锡除它们。
我们将如何界定假？作为第一个猜测，让我们拒绝说明是少于12个字符（太短），
或包含词语"lol"（网络巨魔，煽动性信息）。


----
sightings     = load_sightings();
-- Significant sightings: >= 12 characters, no lulz
sig_sightings = FILTER sightings BY
  ((SIZE(description) >= 12) AND NOT (description MATCHES '(^|.*\W)lol(\W.*|$)'));
----

在大数据探索中一个关键性的行为是总结较大的数据集拆分成可理解的较小数据集。
每个观察集都有一个字段用来给出飞行物体的形状：雪茄、圆盘等等。
下面这个脚本将会告诉我们每个大空船类型有多少次目击：

----
sightings = load_sightings();
craft_sightings = GROUP sightings BY craft;
craft_cts       = FOREACH cf_sightings GENERATE COUNT_STAR(sightings)
STORE craft_cts INTO '$out_dir/craft_cts';
----

我们可以通过修改维基百科上每次目击的文章，制作一些关于发生地点的小旅行指南。
使用联接运算符基于公共密钥的从不同表匹配记录：


----
DEFINE Wikipedify  pigsy.text.Wikipedify;
articled_sightings = JOIN
  articles  BY (wikipedia_id),
  sightings BY (Wikipedify(CONCAT(city, ', ', state))
  USING replicated;
----

其中相对简单的部分：从超过4百万的文章中搜索找到匹配项。
其中相对复杂的部分：准备那个公共密钥。
Pig没有相关的内置功能，但它允许你使用用户定义的函数（User-Defined Functions, UDFs）扩展它的语言。
我们已经启用了这样的UDF——一个函数来准备维基百科的文章编号格式的字符
串——使用` DEFINE`声明。
在第四行，我们合并城市和国家为一个单值，并且执行我们
的` Wikipedify`函数，给出一个匹配记录的通用基础。
其中相对需要技巧的部分：知道什么时候附加 `USING replicated` 声明，
知道在声明中放`articles` 和 `sightings` 的顺序。
正确的选择可能意味执行此查询的速率会有几倍的加速。
这本书将会让你任赖框架去处理简单的部分，加速通过复杂的部分，
并且知道什么时候又因为什么去使用那些需要技巧的部分。


----
TODO: sample output
----

这个旅行指南到目前为止确实有点仅像一个噱头，不过你知道的，
我们目前仅仅在第一章的结尾处。我们可以想出各种方法来改善它。
举例来说，一个适当的引导将不只是在关于大体位置的一篇文章，
而是一系列附近的有趣地方。在这本书的后续部分，我们会告诉你如何做一个
附近的周边查询（在地理数据章节（REF）），
在你知道怎么做之前，这确实是极其困难。
你会立刻发现，找一个未分化的兴趣点列表，几乎比只列一个兴趣更糟糕。
在这本书的后续部分中，我们也将向你展示如何附加一个“突出”的
概念（在事件日志中章（REF））。

让我们一起开始，我们将遇到 猩猩 和 大象，
这些新朋友的冒险看起来正奇怪地迎向我们……


=== 应用范围


* 电子商务
* 生物技术
* 制造业次品
* 安全
* 推荐
* 财政
* 情报

* 缺陷模式 (安全漏洞, 制造业次品, 内部安全,
  - 异常检测
  - 因果分析
* 预言
  - 病人患败血症的可能性
